import os
import pyspark
from pyspark.sql import SparkSession

# Configurações do iceberg
conf = (
    pyspark.SparkConf()
        .setAppName('app_name')
  		# Pacotes
        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.3,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')
  		# Extensões do SQL
        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')
  		# Configurações do catalog
        .set('spark.sql.catalog.iceberg', 'org.apache.iceberg.spark.SparkCatalog')
        .set('spark.sql.catalog.iceberg.type', 'hadoop')
        .set('spark.sql.catalog.iceberg.warehouse', 'iceberg-warehouse')
        .set("spark.sql.catalogImplementation","hive")
)

spark = SparkSession.builder.config(conf=conf).getOrCreate()
print("Spark Running")

# Criar tabela com base no dataset
#df = spark.read.option("header",True).parquet("./parquet-data/flights.parquet")
#df.count()
#df.writeTo("iceberg.flights_data_1m").create()

# Inserir
#df = spark.sql("""INSERT INTO iceberg.flights_data_1m
#(FL_DATE, DEP_DELAY, ARR_DELAY, AIR_TIME, DISTANCE, DEP_TIME, ARR_TIME) VALUES 
#(NOW(), 7, -3, 480, 3711, 13.21, 14.2121)""")

# Atualizar
#df = spark.sql("""UPDATE iceberg.flights_data_1m SET DEP_DELAY = 8 WHERE FL_DATE > '2024-01-01'""")

# Deletar
#df = spark.sql("""DELETE FROM iceberg.flights_data_1m WHERE FL_DATE > '2024-01-01' AND DEP_DELAY = 8""")

# Comando de ler
df = spark.sql("SELECT * FROM iceberg.flights_data_1m WHERE FL_DATE > '2024-01-01' ORDER BY FL_DATE DESC LIMIT 5")

df.show()